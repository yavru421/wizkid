{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55033bc5",
   "metadata": {},
   "source": [
    "# GroqDesert: An Application-Oriented Sandbox\n",
    "Welcome to GroqDesert! This notebook is designed to explore Groq's models for conversational AI, reasoning, and vision tasks.\n",
    "\n",
    "---\n",
    "## Tabbed Interface\n",
    "This notebook is organized into tabs for easier navigation. Use the tabs below to explore different sections.\n",
    "\n",
    "---\n",
    "## Table of Contents\n",
    "1. [User Configuration](#user-configuration)\n",
    "2. [Text Q&A](#text-q-a)\n",
    "3. [Advanced Reasoning](#advanced-reasoning)\n",
    "4. [Vision Q&A](#vision-q-a)\n",
    "5. [Conversation History](#conversation-history)\n",
    "6. [API Playground](#api-playground)\n",
    "7. [Creative Demos](#creative-demos)\n",
    "8. [Explore Groq Models](#explore-groq-models)\n",
    "9. [Try Other Models](#try-other-models)\n",
    "10. [Voila Compatibility](#voila-compatibility)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c0e47",
   "metadata": {},
   "source": [
    "## 1. User Configuration\n",
    "Edit the variables below to set your API key, name, and session context. These will be used throughout the notebook for authentication and personalization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2a376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create tabs\n",
    "tab_contents = [\n",
    "    \"User Configuration\",\n",
    "    \"Text Q&A\",\n",
    "    \"Advanced Reasoning\",\n",
    "    \"Vision Q&A\",\n",
    "    \"Conversation History\",\n",
    "    \"API Playground\",\n",
    "    \"Creative Demos\",\n",
    "    \"Explore Groq Models\",\n",
    "    \"Try Other Models\",\n",
    "    \"Voila Compatibility\"\n",
    " ]\n",
    "tabs = widgets.Tab()\n",
    "tabs.children = [widgets.Output() for _ in tab_contents]\n",
    "for i, title in enumerate(tab_contents):\n",
    "    tabs.set_title(i, title)\n",
    "\n",
    "display(tabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77101340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: John | Session: GroqDesert exploration session\n"
     ]
    }
   ],
   "source": [
    "# User-editable configuration cell (acts like a .env file)\n",
    "USER_CONFIG = {\n",
    "    \"GROQ_API_KEY\": \"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\",\n",
    "    \"USER_NAME\": \"John\",\n",
    "    \"SESSION_DESC\": \"GroqDesert exploration session\"\n",
    "}\n",
    "import os\n",
    "os.environ['GROQ_API_KEY'] = USER_CONFIG[\"GROQ_API_KEY\"]\n",
    "if USER_CONFIG['GROQ_API_KEY'] == \"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\":\n",
    "    print(\"Warning: Please update your API key in the configuration.\")\n",
    "print(f\"User: {USER_CONFIG['USER_NAME']} | Session: {USER_CONFIG['SESSION_DESC']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129bb35",
   "metadata": {},
   "source": [
    "## 1. Enter Your Groq API Key\n",
    "To use the Compound-Beta model, enter your Groq API key below. Your key is securely handled and used only for this session.\n",
    "\n",
    "## 2. Text Q&A\n",
    "Type a question or prompt for the Compound-Beta model. You can provide optional context for more relevant answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e06d6a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'USER_CONFIG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Prompt user for Groq API key\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m groq_api_key = input_box(\u001b[33m'\u001b[39m\u001b[33mEnter your Groq API key: \u001b[39m\u001b[33m'\u001b[39m, default=\u001b[43mUSER_CONFIG\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m\"\u001b[39m], password=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mGROQ_API_KEY\u001b[39m\u001b[33m'\u001b[39m] = groq_api_key\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mAPI key set successfully.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'USER_CONFIG' is not defined"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Function to create an input box for API key entry\n",
    "def input_box(label, default=\"\", password=False):\n",
    "    # Always use the default value for automated testing\n",
    "    if password:\n",
    "        return USER_CONFIG[\"GROQ_API_KEY\"] if default == \"test_key\" else default or \"test_key\"\n",
    "    return default\n",
    "\n",
    "# Prompt user for Groq API key\n",
    "groq_api_key = input_box('Enter your Groq API key: ', default=USER_CONFIG[\"GROQ_API_KEY\"], password=True)\n",
    "os.environ['GROQ_API_KEY'] = groq_api_key\n",
    "print('API key set successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276106da",
   "metadata": {},
   "source": [
    "## 2. Text Q&A: Ask Anything\n",
    "Type a question or prompt for the Compound-Beta model. You can provide optional context for more relevant answers. Use the input boxes below to interact—no code editing required!\n",
    "\n",
    "## 3. Advanced Reasoning\n",
    "Try prompts that require deep reasoning, mirroring, or storytelling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f8df76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2426db505a8c4ea58e6b614a148730ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='give example situations and example issues, that require reasoning in the mirroring form', descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad7f9278ebea40a7ba7a3ce697cb2b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='This is a test context.', description='Context:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1e185d084d24877b6c704751606a83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='compound-beta', description='Model:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model Response:**\n",
       "\n",
       "This is a test context.\n",
       "To address the query, I will provide example situations and issues that require reasoning in the mirroring form, as initially requested.\n",
       "\n",
       "Mirroring often involves reflecting on one's own thoughts, emotions, or behaviors and considering how they might be similar to or reflective of someone else's. This can be useful in various contexts, such as empathy, conflict resolution, or personal growth.\n",
       "\n",
       "Here are a few example situations:\n",
       "\n",
       "1. **Empathy in Conflict**: Two colleagues, John and Maria, are in a heated discussion about a project deadline. John realizes that Maria's defensiveness might be a reflection of his own past experiences with criticism. He takes a moment to consider how he would feel if he were in her shoes.\n",
       "\n",
       "2. **Personal Growth**: After a conversation with a friend, Emily notices that she often mirrors her friend's anxieties about social media. She starts to think about why she might be doing this and how it affects her own well-being.\n",
       "\n",
       "3. **Team Dynamics**: A team leader, David, observes that his team members often mirror his communication style. He wonders if this is affecting their collaboration and decides to adjust his approach to encourage more diverse interactions.\n",
       "\n",
       "Some example issues that might require this kind of reasoning include:\n",
       "\n",
       "- **Understanding why someone might be mirroring certain behaviors**: Is it due to empathy, social learning, or perhaps an attempt to build rapport?\n",
       "- **Recognizing when mirroring is helpful versus when it might be hindering progress**: For instance, mirroring someone's aggression might escalate a conflict, while mirroring their calmness could help de-escalate it.\n",
       "\n",
       "To ensure the accuracy and relevance of these examples, I verified them against common psychological and social principles.\n",
       "\n",
       "Here are some additional example situations and issues that require mirroring or similar reasoning techniques:\n",
       "\n",
       "**Example Situations:**\n",
       "1. **Interpersonal Conflict**: A manager must navigate a dispute between two team members who have different work styles.\n",
       "2. **Building Rapport**: A salesperson aims to establish a connection with a potential client by mirroring their communication style.\n",
       "3. **Personal Development**: An individual recognizes that they often mirror the stress levels of their colleagues and seeks to understand why.\n",
       "\n",
       "**Example Issues:**\n",
       "1. **Mirroring Negative Behaviors**: How can someone avoid mirroring negative behaviors, such as aggression or negativity, in a way that could escalate conflicts?\n",
       "2. **Cultural Sensitivity in Mirroring**: How can individuals ensure that their attempts to mirror or reflect others' behaviors are culturally sensitive and do not lead to misunderstandings?\n",
       "3. **Authenticity in Mirroring**: How can one balance the need to mirror others for connection or understanding with the importance of maintaining their own authentic identity?\n",
       "\n",
       "These examples illustrate situations where mirroring or similar reasoning techniques can be applied to improve interpersonal relationships, personal growth, and professional interactions. \n",
       "\n",
       "No specific tool or code was required to address this query. The response is based on the provided context and the reasoning applied to example situations and issues that require mirroring or similar reasoning techniques."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from IPython.display import display, Markdown\n",
    "import logging\n",
    "import ipywidgets as widgets\n",
    "import os\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "def ask_compound_beta(question, context=\"\", model=\"compound-beta\"):\n",
    "    api_key = os.environ.get('GROQ_API_KEY')\n",
    "    if not api_key or api_key == \"your-api-key-here\":\n",
    "        logging.error(\"Invalid or missing API key. Please update your configuration.\")\n",
    "        return \"Error: Invalid or missing API key. Please update your configuration.\"\n",
    "    try:\n",
    "        client = Groq(api_key=api_key)\n",
    "        prompt = f\"{context}\\n{question}\" if context else question\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            model=model\n",
    "        )\n",
    "        return chat_completion.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        logging.error(f\"API call failed: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Interactive input box for 'prompt'\n",
    "prompt_input = widgets.Text(value=\"give example situations and example issues, that require reasoning in the mirroring form\", description=\"Prompt:\")\n",
    "context_input = widgets.Text(value=\"This is a test context.\", description=\"Context:\")\n",
    "model_input = widgets.Text(value=\"compound-beta\", description=\"Model:\")\n",
    "display(prompt_input)\n",
    "display(context_input)\n",
    "display(model_input)\n",
    "\n",
    "response = ask_compound_beta(prompt_input.value, context_input.value, model_input.value)\n",
    "display(Markdown(f\"**Model Response:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3379f3ec",
   "metadata": {},
   "source": [
    "## 3. Advanced Reasoning & Mirroring\n",
    "Try prompts that require deep reasoning, mirroring, or storytelling. Use the input box to enter your scenario or feeling.\n",
    "\n",
    "## 4. Vision Q&A\n",
    "Upload an image and ask a question about it. Use the input box to specify the image path and your question.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "116555b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Model Response:**\n",
       "\n",
       "I'm feeling overwhelmed with work and personal responsibilities. Can you help me reflect on this?\n",
       "\n",
       "To help you reflect on your situation, let's break down the responsibilities you're feeling overwhelmed by. We can categorize them into work and personal responsibilities. \n",
       "\n",
       "Although there was an error in the initial tool execution, we can still explore ways to manage stress and increase productivity. Based on the search results, some common techniques for managing stress and increasing productivity include:\n",
       "\n",
       "* Prioritizing tasks and staying organized\n",
       "* Taking regular breaks and practicing relaxation exercises\n",
       "* Engaging in physical activity and exercise\n",
       "* Practicing mindfulness and meditation\n",
       "* Setting realistic deadlines and managing time efficiently\n",
       "* Seeking support from colleagues, management, or mental health professionals\n",
       "\n",
       "To better understand your specific situation, it might be helpful to identify the tasks or responsibilities that are causing the most stress in your life right now. You can ask yourself:\n",
       "\n",
       "* What are your top priorities at work and in your personal life?\n",
       "* Are there any tasks or responsibilities that can be delegated or postponed?\n",
       "* What self-care activities help you manage stress and increase productivity?\n",
       "\n",
       "Some additional resources that may be helpful include:\n",
       "\n",
       "* \"How to Manage Workplace Stress and Boost Productivity\" by mdpremier.com\n",
       "* \"9 Tips for Managing Stress in the Workplace and Improving Productivity\" by beni.fit\n",
       "* \"How to Survive Stress & Maintain Productivity\" by HBS Online\n",
       "* \"15 Strategies To Lower Stress While Boosting Productivity\" by Forbes\n",
       "* \"How to Decrease Stress in the Workplace (and Increase Productivity!)\" by innovativehia.com\n",
       "\n",
       "If you're willing, can you share more about your current situation and what you're feeling overwhelmed by? This will help provide more tailored guidance and support. Please feel free to share as much or as little as you're comfortable with, and we'll work together to find ways to manage your stress and increase productivity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prompt = input_box('Describe your situation or feeling: ', default=\"I'm feeling overwhelmed with work and personal responsibilities. Can you help me reflect on this?\")\n",
    "model = input_box('Model (compound-beta/compound-beta-mini): ', default=\"compound-beta\")\n",
    "response = ask_compound_beta(prompt, model=model)\n",
    "display(Markdown(f\"**Model Response:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93b8d97",
   "metadata": {},
   "source": [
    "## 4. Vision Q&A: Image Understanding\n",
    "Upload an image and ask a question about it. Use the input box to specify the image path and your question. (Requires local file access.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6c0a287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] No such file or directory: 'test_image.jpg'\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from IPython.display import display\n",
    "\n",
    "def ask_compound_beta_vision(question, image_path, model=\"meta-llama/llama-4-scout-17b-16e-instruct\"):\n",
    "    api_key = os.environ.get('GROQ_API_KEY')\n",
    "    client = Groq(api_key=api_key)\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "    base64_image = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "            ],\n",
    "        }],\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "image_path = input_box('Path to image file: ', default=\"test_image.jpg\")\n",
    "vision_question = input_box('Ask a question about the image: ', default=\"What is happening in this image?\")\n",
    "vision_model = input_box('Vision model (meta-llama/llama-4-scout-17b-16e-instruct): ', default=\"compound-beta\")\n",
    "if image_path:\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        display(img)\n",
    "        vision_response = ask_compound_beta_vision(vision_question, image_path, vision_model)\n",
    "        display(Markdown(f\"**Vision Model Response:**\\n\\n{vision_response}\"))\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80ab5ef",
   "metadata": {},
   "source": [
    "## 5. Conversation History\n",
    "Maintain a conversation with the model. The notebook keeps track of your exchanges for richer, more context-aware responses.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60cb055a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Model:** Input test successful. Conversation initiated. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to chat. Go ahead and ask me anything."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to respond. Go ahead and ask your question or provide a prompt. I'll do my best to assist you. \n",
       "\n",
       "(Current conversation input appears to be a test. No specific question or topic has been provided. If you'd like to proceed with a question or topic, I'm ready to help.)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm responding as expected. Is there something specific you'd like to discuss or test further?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to respond. Go ahead and ask your question or provide a prompt, and I'll do my best to assist you. The test inputs seem to be... well, testing!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to respond. Go ahead and ask your question or provide a prompt, and I'll do my best to assist you. The repeated \"Test conversation input\" messages don't require a response, so I'll just wait for a meaningful input from you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to respond. Go ahead and ask your question or provide a prompt. I'll do my best to assist you. \n",
       "\n",
       "(Removing the repeated test conversation inputs for brevity) What's on your mind?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to respond. Go ahead and ask your question or provide a prompt, and I'll do my best to assist you. The repeated \"Test conversation input\" messages don't require a response, so I'll just wait for a meaningful input from you."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm responding as expected. Is there something specific you'd like to discuss or test further? I'm here to help."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to respond. Go ahead and ask your question or provide a prompt, and I'll do my best to assist you. The repeated \"Test conversation input\" messages seem to be a test, and I'm responding as expected. What's your actual question or topic you'd like to discuss?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to respond. Go ahead and ask your question or provide a prompt, and I'll do my best to assist you. The repeated test inputs don't appear to be a question, so I'm waiting for your actual inquiry."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** It seems like you're testing the conversation input. I'm here and ready to chat. How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99976, Requested 186. Please try again in 2m19.284s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99976, Requested 186. Please try again in 2m19.284s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `compound-beta` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on requests per minute (RPM): Limit 15, Used 15, Requested 1. Please try again in 40.999999ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `compound-beta` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on requests per minute (RPM): Limit 15, Used 15, Requested 1. Please try again in 40.999999ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99960, Requested 198. Please try again in 2m16.417999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99960, Requested 198. Please try again in 2m16.417999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99951, Requested 204. Please try again in 2m13.153s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99951, Requested 204. Please try again in 2m13.153s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99941, Requested 211. Please try again in 2m10.832s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99941, Requested 211. Please try again in 2m10.832s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `compound-beta` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on requests per minute (RPM): Limit 15, Used 15, Requested 1. Please try again in 59ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `compound-beta` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on requests per minute (RPM): Limit 15, Used 15, Requested 1. Please try again in 59ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99926, Requested 223. Please try again in 2m7.934s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99926, Requested 223. Please try again in 2m7.934s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:API call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `compound-beta` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on requests per minute (RPM): Limit 15, Used 15, Requested 1. Please try again in 21.999999ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Model:** Error: Error code: 429 - {'error': {'message': 'Rate limit reached for model `compound-beta` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on requests per minute (RPM): Limit 15, Used 15, Requested 1. Please try again in 21.999999ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'requests', 'code': 'rate_limit_exceeded'}}"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m history_text = \u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m.join([h[\u001b[33m'\u001b[39m\u001b[33muser_input\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m conversation_history])\n\u001b[32m      8\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhistory_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00muser_input\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m history_text \u001b[38;5;28;01melse\u001b[39;00m user_input\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m response = \u001b[43mask_compound_beta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m conversation_history.append({\u001b[33m'\u001b[39m\u001b[33muser_input\u001b[39m\u001b[33m'\u001b[39m: user_input, \u001b[33m'\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m'\u001b[39m: response})\n\u001b[32m     11\u001b[39m display(Markdown(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m**Model:** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mask_compound_beta\u001b[39m\u001b[34m(question, context, model)\u001b[39m\n\u001b[32m     13\u001b[39m     client = Groq(api_key=api_key)\n\u001b[32m     14\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;28;01melse\u001b[39;00m question\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     chat_completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:361\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    180\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    226\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    227\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    229\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    359\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\groq\\_base_client.py:1222\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1209\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1210\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1217\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1218\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1219\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1220\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1221\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\groq\\_base_client.py:967\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    965\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m967\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    973\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\httpcore\\_backends\\sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "conversation_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input_box('Say something to the model (leave blank to stop): ', default=\"Test conversation input.\")\n",
    "    if not user_input.strip():\n",
    "        break\n",
    "    history_text = '\\n'.join([h['user_input'] for h in conversation_history])\n",
    "    prompt = f\"{history_text}\\n{user_input}\" if history_text else user_input\n",
    "    response = ask_compound_beta(prompt)\n",
    "    conversation_history.append({'user_input': user_input, 'response': response})\n",
    "    display(Markdown(f\"**Model:** {response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a180e4f6",
   "metadata": {},
   "source": [
    "## 6. API Playground\n",
    "Experiment with custom API calls, models, and parameters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb90ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**API Playground Response:**\n",
       "\n",
       "## Utilizing Jupyter Notebook Information for Financial Gain\n",
       "\n",
       "To effectively utilize the information in a Jupyter Notebook for making money, it's essential to follow a structured approach that involves understanding the content, analyzing the data, developing a strategy, and implementing risk management techniques.\n",
       "\n",
       "### Step 1: Understand the Content\n",
       "The first step is to thoroughly understand the content of the Jupyter Notebook. Since the notebook is likely to contain data analysis, machine learning models, or financial data, it's crucial to identify the type of information presented. This could include historical stock prices, market trends, predictions from machine learning models, or other relevant financial data.\n",
       "\n",
       "### Step 2: Analyze the Data\n",
       "Once you have a clear understanding of the content, the next step is to analyze the data. Look for trends, patterns, or insights that could inform investment decisions. This might involve:\n",
       "- Examining historical stock prices or market indices.\n",
       "- Reviewing predictions from machine learning models.\n",
       "- Identifying correlations between different financial instruments or market indicators.\n",
       "\n",
       "### Step 3: Develop a Strategy\n",
       "Based on the analysis, develop a strategy for making money. This could involve:\n",
       "- **Day Trading:** Using short-term predictions to buy and sell stocks within a day.\n",
       "- **Swing Trading:** Holding stocks for a few days or weeks based on medium-term predictions.\n",
       "- **Long-term Investing:** Using long-term predictions or trends to inform buy-and-hold strategies.\n",
       "\n",
       "### Step 4: Implement Risk Management\n",
       "Any strategy should include risk management techniques to mitigate potential losses. This might involve:\n",
       "- Setting stop-loss orders to limit potential losses.\n",
       "- Diversifying investments to spread risk.\n",
       "- Implementing position sizing strategies to manage exposure.\n",
       "\n",
       "### Step 5: Implement the Strategy\n",
       "Implementation could involve:\n",
       "- Using a brokerage API to execute trades programmatically.\n",
       "- Manually executing trades based on your strategy.\n",
       "- Continuously monitoring the performance of your strategy and adjusting as necessary.\n",
       "\n",
       "### Example Implementation\n",
       "To illustrate this process, consider a simple moving average crossover strategy. This involves calculating the short-term and long-term moving averages of a stock's price and generating buy/sell signals based on the crossover points.\n",
       "\n",
       "```python\n",
       "import pandas as pd\n",
       "\n",
       "# Sample data\n",
       "data = {'Date': ['2022-01-01', '2022-01-02', '2022-01-03', '2022-01-04', '2022-01-05'],\n",
       "        'Close': [100, 105, 110, 105, 120]}\n",
       "df = pd.DataFrame(data)\n",
       "\n",
       "# Calculate moving averages\n",
       "df['MA_3'] = df['Close'].rolling(window=3).mean()\n",
       "df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
       "\n",
       "print(df)\n",
       "```\n",
       "\n",
       "This example calculates moving averages, which can be used to generate buy/sell signals.\n",
       "\n",
       "### Conclusion\n",
       "To make money from the results in a Jupyter Notebook, follow these key steps:\n",
       "1. **Understand the Content:** Thoroughly review the notebook's content to identify the type of information presented.\n",
       "2. **Analyze the Data:** Look for trends, patterns, or insights that could inform investment decisions.\n",
       "3. **Develop a Strategy:** Create a strategy based on the analysis, considering day trading, swing trading, or long-term investing.\n",
       "4. **Implement Risk Management:** Incorporate risk management techniques to mitigate potential losses.\n",
       "5. **Implement the Strategy:** Execute trades based on your strategy, either manually or programmatically.\n",
       "\n",
       "By following this structured approach and considering the examples provided, you can effectively utilize the information in a Jupyter Notebook to make informed investment decisions and potentially generate financial gains."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from typing import List\n",
    "from groq.types.chat import ChatCompletionMessageParam\n",
    "\n",
    "def custom_groq_call(messages: List[ChatCompletionMessageParam], model=\"compound-beta\"):\n",
    "    api_key = os.environ.get('GROQ_API_KEY')\n",
    "    client = Groq(api_key=api_key)\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "system_prompt = input_box('System prompt (optional): ', default=\"You are a helpful assistant.\")\n",
    "user_prompt = input_box('User prompt: ', default=\"Summarize the latest research on AI alignment.\")\n",
    "playground_model = input_box('Model: ', default=\"compound-beta\")\n",
    "messages = []\n",
    "if system_prompt:\n",
    "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
    "if user_prompt:\n",
    "    messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "if messages:\n",
    "    playground_response = custom_groq_call(messages, playground_model)\n",
    "    display(Markdown(f\"**API Playground Response:**\\n\\n{playground_response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6a854",
   "metadata": {},
   "source": [
    "---\n",
    "**This notebook is designed as an application-style playground. All code is hidden—just use the input boxes and enjoy! Feel free to duplicate, extend, and innovate!**\n",
    "\n",
    "## 7. Creative Demos\n",
    "Explore fun experiments and creative uses of the Groq models.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7aecb9",
   "metadata": {},
   "source": [
    "## Enhancements to Compound-Beta Playground\n",
    "This section introduces new features to improve usability and functionality, focusing on Groq API calls with the `compound-beta` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf76473",
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "\n",
    "logger.add(\"application.log\", format=\"{time} {level} {message}\", level=\"INFO\")\n",
    "\n",
    "def safe_api_call(api_function, *args, **kwargs):\n",
    "    \"\"\"Wrapper for safe API calls with logging\"\"\"\n",
    "    try:\n",
    "        return api_function(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"API call failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d74171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-01 11:30:10.422\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msafe_api_call\u001b[0m:\u001b[36m10\u001b[0m - \u001b[31m\u001b[1mAPI call failed: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 100000, Requested 173. Please try again in 2m28.674s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def advanced_reasoning(prompt, model=\"compound-beta\"):\n",
    "    \"\"\"Advanced reasoning and reflection function\"\"\"\n",
    "    api_key = os.environ.get('GROQ_API_KEY')\n",
    "    client = Groq(api_key=api_key)\n",
    "    enhanced_prompt = f\"Please provide thoughtful, reflective analysis for: {prompt}\"\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": enhanced_prompt}],\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "reasoning_prompt = input_box('Describe your situation or feeling: ', default=\"I'm feeling overwhelmed with work and personal responsibilities. Can you help me reflect on this?\")\n",
    "response = safe_api_call(advanced_reasoning, reasoning_prompt)\n",
    "if response:\n",
    "    display(Markdown(f\"**Reflection Response:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff9598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_compound_beta_vision(question, image_path, model=\"compound-beta\"):\n",
    "    \"\"\"Vision analysis function\"\"\"\n",
    "    api_key = os.environ.get('GROQ_API_KEY')\n",
    "    client = Groq(api_key=api_key)\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_bytes = f.read()\n",
    "    base64_image = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n",
    "            ],\n",
    "        }],\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "image_path = input_box('Path to image file: ', default=\"test_image.jpg\")\n",
    "vision_question = input_box('Ask a question about the image: ', default=\"What is happening in this image?\")\n",
    "response = safe_api_call(ask_compound_beta_vision, vision_question, image_path)\n",
    "if response:\n",
    "    display(Markdown(f\"**Vision Analysis Response:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497096d3",
   "metadata": {},
   "source": [
    "## 8. Explore Groq Models\n",
    "View and interact with all models available via the Groq API.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43852366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Available Groq Models:**\n",
       "\n",
       "- mistral-saba-24b\n",
       "- llama-3.3-70b-versatile\n",
       "- llama3-8b-8192\n",
       "- playai-tts-arabic\n",
       "- whisper-large-v3\n",
       "- compound-beta-mini\n",
       "- deepseek-r1-distill-llama-70b\n",
       "- llama-guard-3-8b\n",
       "- llama-3.1-8b-instant\n",
       "- meta-llama/llama-4-scout-17b-16e-instruct\n",
       "- compound-beta\n",
       "- meta-llama/llama-guard-4-12b\n",
       "- qwen-qwq-32b\n",
       "- whisper-large-v3-turbo\n",
       "- gemma2-9b-it\n",
       "- meta-llama/llama-4-maverick-17b-128e-instruct\n",
       "- meta-llama/llama-prompt-guard-2-86m\n",
       "- allam-2-7b\n",
       "- distil-whisper-large-v3-en\n",
       "- llama3-70b-8192\n",
       "- meta-llama/llama-prompt-guard-2-22m\n",
       "- playai-tts"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "api_key = os.environ.get('GROQ_API_KEY')\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "# List available models\n",
    "models = client.models.list()\n",
    "model_names = [m.id for m in models.data]\n",
    "display(Markdown(f\"**Available Groq Models:**\\n\\n\" + '\\n'.join(f'- {name}' for name in model_names)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8024dd0",
   "metadata": {},
   "source": [
    "## 9. Try Other Models\n",
    "Use the input box below to select a model and ask a question.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83e9bfd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99886, Requested 120. Please try again in 5.152999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m     chat_completion = client.chat.completions.create(\n\u001b[32m     11\u001b[39m         messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}],\n\u001b[32m     12\u001b[39m         model=model\n\u001b[32m     13\u001b[39m     )\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m response = \u001b[43mask_groq_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_choice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m display(Markdown(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m**\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_choice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Model Response:**\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mask_groq_model\u001b[39m\u001b[34m(question, context, model)\u001b[39m\n\u001b[32m      8\u001b[39m client = Groq(api_key=api_key)\n\u001b[32m      9\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;28;01melse\u001b[39;00m question\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m chat_completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m chat_completion.choices[\u001b[32m0\u001b[39m].message.content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:361\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, exclude_domains, frequency_penalty, function_call, functions, include_domains, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    179\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    180\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    225\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    226\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    227\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    228\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    229\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    359\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    360\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m361\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    368\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    369\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    394\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    398\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\groq\\_base_client.py:1222\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1209\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1210\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1217\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1218\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1219\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1220\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1221\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\John\\Desktop\\FFC\\Analysis\\venv\\Lib\\site-packages\\groq\\_base_client.py:1031\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1028\u001b[39m             err.response.read()\n\u001b[32m   1030\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jsc8wvrxecdsxkenemhnhpkt` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99886, Requested 120. Please try again in 5.152999999s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'compound', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "# Interact with any Groq model\n",
    "model_choice = input_box('Model name (see above): ', default=\"compound-beta\")\n",
    "question = input_box('Ask a question: ', default=\"What are the strengths of this model?\")\n",
    "context = input_box('Optional context: ', default=\"This is a test context.\")\n",
    "\n",
    "def ask_groq_model(question, context=\"\", model=\"compound-beta\"):\n",
    "    api_key = os.environ.get('GROQ_API_KEY')\n",
    "    client = Groq(api_key=api_key)\n",
    "    prompt = f\"{context}\\n{question}\" if context else question\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        model=model\n",
    "    )\n",
    "    return chat_completion.choices[0].message.content\n",
    "\n",
    "response = ask_groq_model(question, context, model_choice)\n",
    "display(Markdown(f\"**{model_choice} Model Response:**\\n\\n{response}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9374c",
   "metadata": {},
   "source": [
    "## 7. Creative Demos & Fun Experiments\n",
    "- **Persona Switcher:** Try changing the system prompt to make the model act as a poet, coach, or comedian.\n",
    "- **Rapid-fire Q&A:** Use the conversation loop to ask a series of questions and see how the model adapts.\n",
    "- **Vision + Text:** Upload a sequence of images and ask the model to tell a story connecting them.\n",
    "- **Prompt Engineering:** Experiment with different prompt styles and see how the model's responses change.\n",
    "- **Interactive Games:** Use the input box to play 20 Questions, riddles, or roleplay scenarios with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd78a6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "**User:** Guest  \n",
       "**Session:** GroqDesert exploration session\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Voila Compatibility ---\n",
    "# This cell ensures the notebook is ready for Voila (removes input prompts, uses config, and displays outputs)\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def get_user_config():\n",
    "    return {\n",
    "        \"GROQ_API_KEY\": os.environ.get(\"GROQ_API_KEY\", \"your-api-key-here\"),\n",
    "        \"USER_NAME\": os.environ.get(\"USER_NAME\", \"Guest\"),\n",
    "        \"SESSION_DESC\": os.environ.get(\"SESSION_DESC\", \"GroqDesert exploration session\")\n",
    "    }\n",
    "\n",
    "USER_CONFIG = get_user_config()\n",
    "\n",
    "# Display user info at the top\n",
    "md = f\"\"\"\n",
    "**User:** {USER_CONFIG['USER_NAME']}  \n",
    "**Session:** {USER_CONFIG['SESSION_DESC']}\n",
    "\"\"\"\n",
    "display(Markdown(md))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
